/*
 * AUTOGENERATED tcarstens January 2014
 */
#ifndef __UL64_0__
#define __UL64_0__

#include "../stdint.hl"
#include "../rng.hl"


/*
 * ul64
 */
typedef struct ul64_s {
    uint32_t x[2];
} ul64[1];

inline void ul64_init(ul64 x) { return; }
inline void ul64_clear(ul64 x) { return; }

typedef struct mod64_s {
    ul64 n;
    uint32_t np;
    ul64 rsq;
} mod64[1];




/*
 * Setters
 */
inline void ul64_set_gp(__global ul64 dst, ul64 src) {
    dst->x[0] = src->x[0];
    dst->x[1] = src->x[1];
}
inline void ul64_set_pg(ul64 dst, __global ul64 src) {
    dst->x[0] = src->x[0];
    dst->x[1] = src->x[1];
}
inline void ul64_set_gg(__global ul64 dst, __global ul64 src) {
    dst->x[0] = src->x[0];
    dst->x[1] = src->x[1];
}
inline void ul64_set(ul64 dst, ul64 src) {
    dst->x[0] = src->x[0];
    dst->x[1] = src->x[1];
}
/*
 * Set a ul64 to a uint32_t
 */
inline void ul64_set_ui(ul64 dst, uint32_t i) {
    dst->x[0] = i;
    dst->x[1] = 0;
}

/*
 * Get a uint32_t out of a ul64
 */
inline uint32_t ul64_get_ui(ul64 src) {
    return src->x[0];
}




/*
 * Generate a random ul64
 */
inline void ul64_rand(struct rng_t *r, ul64 dst) {
    uint64_t w0 = rand_uint64(r);
    
    dst->x[0] = w0 & 0xffffffff;
    dst->x[1] = w0  >> 32;
}



/*
 * Compare two ul64's
 */
inline int ul64_cmp(ul64 src1, ul64 src2) {
    int r = 0;
    if (src1->x[1] > src2->x[1]) r = 1;
    else if (src1->x[1] < src2->x[1]) r = -1;
    else if (src1->x[0] > src2->x[0]) r = 1;
    else if (src1->x[0] < src2->x[0]) r = -1;
    return r;
}

/*
 * Compare a ul64 with a uint32_t
 */
inline int ul64_cmp_ui(ul64 src1, uint32_t src2) {
    int r = 0;
    if ( src1->x[1]) r = 1;
    else if (src1->x[0] > src2) r = 1;
    else if (src1->x[0] < src2) r = -1;
    return r;
}




/*
 * Add two ul64's
 */
inline void ul64_add(ul64 dst, ul64 src1, ul64 src2) {
    #if defined(UL_NVIDIA)
        asm(
          "add.cc.u32  %0, %2, %4;\n\t"
          "addc.u32    %1, %3, %5;\n\t"
          : "=r" (dst->x[0]), "=r" (dst->x[1])
          : "r" (src1->x[0]), "r" (src1->x[1]), 
            "r" (src2->x[0]), "r" (src2->x[1])
          : "cc"
        );
    #else
        ul64 d = { 0 };
        d->x[0] = (src1->x[0] & 0x7fffffff) + (src2->x[0] & 0x7fffffff) + 0;
        uint32_t c0 = (src1->x[0] >> 31) + (src2->x[0] >> 31) + (d->x[0] >> 31);
        d->x[0] = (c0 << 31) | (d->x[0] & 0x7fffffff);
        c0 = c0 >> 1;
        dst->x[0] = d->x[0];
        
        d->x[1] = (src1->x[1] & 0x7fffffff) + (src2->x[1] & 0x7fffffff) + c0;
        uint32_t c1 = (src1->x[1] >> 31) + (src2->x[1] >> 31) + (d->x[1] >> 31);
        d->x[1] = (c1 << 31) | (d->x[1] & 0x7fffffff);
        c1 = c1 >> 1;
        dst->x[1] = d->x[1];
        
    #endif
    return;
}
/*
 * Sub two ul64's
 */
inline void ul64_sub(ul64 dst, ul64 src1, ul64 src2) {
    #if defined(UL_NVIDIA)
        asm(
          "sub.cc.u32  %0, %2, %4;\n\t"
          "subc.u32    %1, %3, %5;\n\t"
          : "=r" (dst->x[0]), "=r" (dst->x[1])
          : "r" (src1->x[0]), "r" (src1->x[1]), 
            "r" (src2->x[0]), "r" (src2->x[1])
          : "cc"
        );
    #else
        ul64 d = { 0 };
        d->x[0] = (src1->x[0] & 0x7fffffff) - (src2->x[0] & 0x7fffffff) - 0;
        uint32_t b0 = (src1->x[0] >> 31) - (src2->x[0] >> 31) - (d->x[0] >> 31);
        d->x[0] = (b0 << 31) | (d->x[0] & 0x7fffffff);
        b0 = (b0 >> 1) & 1;
        dst->x[0] = d->x[0];
        
        d->x[1] = (src1->x[1] & 0x7fffffff) - (src2->x[1] & 0x7fffffff) - b0;
        uint32_t b1 = (src1->x[1] >> 31) - (src2->x[1] >> 31) - (d->x[1] >> 31);
        d->x[1] = (b1 << 31) | (d->x[1] & 0x7fffffff);
        b1 = (b1 >> 1) & 1;
        dst->x[1] = d->x[1];
        
    #endif
    return;
}
/*
 * Mul two ul64's
 */
inline void ul64_mul(ul64 dst, ul64 src1, ul64 src2) {
    #if defined(UL_NVIDIA)
        asm(
          "mul.lo.u32 %0, %2, %4;\n\t"
          "mul.hi.u32 %1, %2, %4;\n\t"
          "mad.lo.u32 %1, %2, %5, %1;\n\t"
          "mad.lo.u32 %1, %3, %4, %1;\n\t"
          : "=r" (dst->x[0]), "=r" (dst->x[1])
          : "r" (src1->x[0]), "r" (src1->x[1]), 
            "r" (src2->x[0]), "r" (src2->x[1])
          : "cc"
        );
    #else
        uint64_t d = ((((uint64_t)src1->x[1]) << 32) | src1->x[0]) * ((((uint64_t)src2->x[1]) << 32) | src2->x[0]);
        dst->x[0] = d;
        dst->x[1] = d >> 32;
    #endif
    return;
}




/*
 * Initialize mod64
 */
inline void mod64_init(mod64 n) {
}

/*
 * Add two ul64's modulo another
 */
inline void ul64_modadd(ul64 dst, ul64 src1, ul64 src2, mod64 n) {
    ul64_add(dst, src1, src2);
    if (ul64_cmp(dst, n->n) >= 0)
        ul64_sub(dst, dst, n->n);
}

/*
 * Subtract one ul64 from another modulo a third
 */
inline void ul64_modsub(ul64 dst, ul64 src1, ul64 src2, mod64 n) {
    ul64 tr1, tr2;
    ul64_sub(tr1, src1, src2);
    ul64_add(tr2, tr1, n->n);
    if (ul64_cmp(src1, src2) >= 0)
        ul64_set(dst, tr1);
    else
        ul64_set(dst, tr2);
}

/*
 * Mul two ul64's modulo a third, followed by Montgomery reduction
 */
void ul64_modmul(ul64 _dst, ul64 _src1, ul64 _src2, mod64 n) {
    #if defined(UL_NVIDIA)
        volatile ul64 src1;
        volatile ul64 src2;
        /* ul64_set(src1, _src1); */
        src1->x[0] = _src1->x[0];
        src1->x[1] = _src1->x[1];
        /* ul64_set(src2, _src2); */
        src2->x[0] = _src2->x[0];
        src2->x[1] = _src2->x[1];
        
        uint32_t q = 0;
        ul64 dst = { 0 };
        uint32_t dst_2 = 0;
        
        asm(
          /* Compute c_0..1 for the product a*b, with carry-out to dst_2 */
          "mad.lo.cc.u32  %0, %4, %6, %0;\n\t"  /* c_0 += lo(a_0, b_0) */
          "madc.hi.cc.u32 %1, %4, %6, %1;\n\t"  /* c_1 += hi(a_0, b_0) */
          "addc.u32 %2, %2, 0;\n\t"  /* accum carry in c_2 */
          "mad.lo.cc.u32  %1, %4, %7, %1;\n\t"  /* c_1 += lo(a_0, b_1) */
          "addc.u32 %2, %2, 0;\n\t"  /* accum carry in c_2 */
          "mad.lo.cc.u32  %1, %5, %6, %1;\n\t"  /* c_1 += lo(a_1, b_0) */
          "addc.u32 %2, %2, 0;\n\t"  /* accum carry in c_2 */
          
          /* Add in the qN's */
          /* n = 0... */
            /* Compute q = mu * c_0 */
            "mov.u32 %3, %0;\n\t"
            "mul.lo.u32 %3, %3, %10;\n\t"
            /* Update c_0 with qN_0 */
            "mad.lo.cc.u32 %0, %3, %8, %0;\n\t"  /* c_0 += lo(q, n_0) */
            /* Shift */
            "mov.u32 %0, %1;\n\t"  /* dst_0 <- c_1 */
            "mov.u32 %1, %2;\n\t"  /* dst_1 <- c_2 */
            "xor.b32 %2, %2, %2;\n\t"  /* dst_2 <- c_3 */
            /* Compute and add-in qN, with carry-out to dst_2 */
            "madc.hi.cc.u32 %0, %3, %8, %0;\n\t"  /* c_1 += hi(q, n_0) */
            "addc.cc.u32 %1, %1, 0;\n\t"  /* accum carry in dst_1 */
            "addc.u32    %2, %2, 0;\n\t"  /* accum carry in dst_2 */
            "mad.lo.cc.u32  %0, %3, %9, %0;\n\t"  /* c_1 += lo(q, n_1) */
            "madc.hi.cc.u32 %1, %3, %9, %1;\n\t"  /* c_2 += hi(q, n_1) */
            "addc.u32    %2, %2, 0;\n\t"  /* accum carry in dst_2 */
          /* n = 1... */
            /* Compute q = mu * c_1 */
            "mov.u32 %3, %0;\n\t"
            "mul.lo.u32 %3, %3, %10;\n\t"
            /* Update c_1 with qN_0 */
            "mad.lo.cc.u32 %0, %3, %8, %0;\n\t"  /* c_1 += lo(q, n_0) */
            /* Shift */
            "mov.u32 %0, %1;\n\t"  /* dst_0 <- c_2 */
            "mov.u32 %1, %2;\n\t"  /* dst_1 <- c_3 */
            "xor.b32 %2, %2, %2;\n\t"  /* dst_2 <- c_4 */
            /* Compute and add-in qN, with carry-out to dst_2 */
            "madc.hi.cc.u32 %0, %3, %8, %0;\n\t"  /* c_2 += hi(q, n_0) */
            "addc.cc.u32 %1, %1, 0;\n\t"  /* accum carry in dst_1 */
            "addc.u32    %2, %2, 0;\n\t"  /* accum carry in dst_2 */
            "mad.lo.cc.u32  %0, %3, %9, %0;\n\t"  /* c_1 += lo(q, n_1) */
            "madc.hi.cc.u32 %1, %3, %9, %1;\n\t"  /* c_2 += hi(q, n_1) */
            "addc.u32    %2, %2, 0;\n\t"  /* accum carry in dst_2 */
          
          /* Compute c_2..3 in the product a*b, storing the result in dst_0..1, with carry-out to dst_2 */
          "mad.hi.cc.u32  %0, %4, %7, %0;\n\t"  /* c_2 += hi(a_0, b_1) */
          "addc.cc.u32 %1, %1, 0;\n\t"  /* accum carry in c_3 */
          "addc.u32    %2, %2, 0;\n\t"  /* accum carry in c_4 */
          "mad.hi.cc.u32  %0, %5, %6, %0;\n\t"  /* c_2 += hi(a_1, b_0) */
          "addc.cc.u32 %1, %1, 0;\n\t"  /* accum carry in c_3 */
          "addc.u32    %2, %2, 0;\n\t"  /* accum carry in c_4 */
          "mad.lo.cc.u32  %0, %5, %7, %0;\n\t"  /* c_2 += lo(a_1, b_1) */
          "madc.hi.cc.u32 %1, %5, %7, %1;\n\t"  /* c_3 += hi(a_1, b_1) */
          "addc.u32    %2, %2, 0;\n\t"  /* accum carry in c_4 */
          
          : "+r" (dst->x[0]), "+r" (dst->x[1]), "+r" (dst_2), "+r" (q)
          : "r" (src1->x[0]), "r" (src1->x[1]), 
            "r" (src2->x[0]), "r" (src2->x[1]), 
            "r" (n->n->x[0]), "r" (n->n->x[1]), 
            "r" (n->np)
        );
        
        /* Reduce as needed */
        if (dst_2 || (ul64_cmp(dst, n->n) >= 0))
            ul64_sub(dst, dst, n->n);
        ul64_set(_dst, dst);
    #else
        /* The pairwise products of the a_i and b_j */
        const uint64_t a_0__b_0 = ((uint64_t)_src1->x[0]) * ((uint64_t)_src2->x[0]);
        const uint32_t a_0__b_0_lo = a_0__b_0;
        const uint32_t a_0__b_0_hi = a_0__b_0 >> 32;
        const uint64_t a_0__b_1 = ((uint64_t)_src1->x[0]) * ((uint64_t)_src2->x[1]);
        const uint32_t a_0__b_1_lo = a_0__b_1;
        const uint32_t a_0__b_1_hi = a_0__b_1 >> 32;
        const uint64_t a_1__b_0 = ((uint64_t)_src1->x[1]) * ((uint64_t)_src2->x[0]);
        const uint32_t a_1__b_0_lo = a_1__b_0;
        const uint32_t a_1__b_0_hi = a_1__b_0 >> 32;
        const uint64_t a_1__b_1 = ((uint64_t)_src1->x[1]) * ((uint64_t)_src2->x[1]);
        const uint32_t a_1__b_1_lo = a_1__b_1;
        const uint32_t a_1__b_1_hi = a_1__b_1 >> 32;
        
        /* Limbs of the product C = A*B */
        uint32_t c_0 = 0;
        uint32_t q_0 = 0;
        uint32_t c_1 = 0;
        uint32_t q_1 = 0;
        uint32_t c_2 = 0;
        uint32_t q_2 = 0;
        uint32_t c_3 = 0;
        uint32_t q_3 = 0;
        uint32_t c_4 = 0;
        uint32_t q_4 = 0;
        
        /* The product of the q_i's with N */
        uint64_t q_0__N_0 = 0;
        uint64_t q_0__N_1 = 0;
        uint64_t q_1__N_0 = 0;
        uint64_t q_1__N_1 = 0;
        
        /* Compute c_0 */
        {
            /* Add the product terms into c_0, accumulating carries in c_1 */
            c_1 += ul32_addc(&c_0, &c_0, &a_0__b_0_lo);
            
            /* Add the q_i*N's for i < 0 */
            
            /* Compute q_0 and add its product with N */
            q_0 = n->np * c_0;
            q_0__N_0 = ((uint64_t)q_0) * ((uint64_t)n->n->x[0]);
            q_0__N_1 = ((uint64_t)q_0) * ((uint64_t)n->n->x[1]);
            const uint32_t q_0__N_0_lo = q_0__N_0;
            c_1 += ul32_addc(&c_0, &c_0, &q_0__N_0_lo);
        }
        
        /* Compute c_1 */
        {
            /* Add the product terms into c_1, accumulating carries in c_2 */
            c_2 += ul32_addc(&c_1, &c_1, &a_0__b_0_hi);
            c_2 += ul32_addc(&c_1, &c_1, &a_0__b_1_lo);
            c_2 += ul32_addc(&c_1, &c_1, &a_1__b_0_lo);
            
            /* Add the q_i*N's for i < 1 */
            const uint32_t q_0__N_1_lo = q_0__N_1;
            c_2 += ul32_addc(&c_1, &c_1, &q_0__N_1_lo);
            const uint32_t q_0__N_0_hi = q_0__N_0 >> 32;
            c_2 += ul32_addc(&c_1, &c_1, &q_0__N_0_hi);
            
            /* Compute q_1 and add its product with N */
            q_1 = n->np * c_1;
            q_1__N_0 = ((uint64_t)q_1) * ((uint64_t)n->n->x[0]);
            q_1__N_1 = ((uint64_t)q_1) * ((uint64_t)n->n->x[1]);
            const uint32_t q_1__N_0_lo = q_1__N_0;
            c_2 += ul32_addc(&c_1, &c_1, &q_1__N_0_lo);
        }
        
        /* Compute c_2 */
        {
            /* Add the product terms into c_2, accumulating carries in c_3 */
            c_3 += ul32_addc(&c_2, &c_2, &a_0__b_1_hi);
            c_3 += ul32_addc(&c_2, &c_2, &a_1__b_0_hi);
            c_3 += ul32_addc(&c_2, &c_2, &a_1__b_1_lo);
            
            /* Add the q_i*N's for i < 2 */
            const uint32_t q_0__N_1_hi = q_0__N_1 >> 32;
            c_3 += ul32_addc(&c_2, &c_2, &q_0__N_1_hi);
            const uint32_t q_1__N_1_lo = q_1__N_1;
            c_3 += ul32_addc(&c_2, &c_2, &q_1__N_1_lo);
            const uint32_t q_1__N_0_hi = q_1__N_0 >> 32;
            c_3 += ul32_addc(&c_2, &c_2, &q_1__N_0_hi);
            
        }
        
        /* Compute c_3 */
        {
            /* Add the product terms into c_3, accumulating carries in c_4 */
            c_4 += ul32_addc(&c_3, &c_3, &a_1__b_1_hi);
            
            /* Add the q_i*N's for i < 3 */
            const uint32_t q_1__N_1_hi = q_1__N_1 >> 32;
            c_4 += ul32_addc(&c_3, &c_3, &q_1__N_1_hi);
            
        }
        
        /* R = C * beta^{-n} */
        _dst->x[0] = c_2;
        _dst->x[1] = c_3;
        
        /* Reduce as needed */
        if (c_4 || (ul64_cmp(_dst, n->n) >= 0))
            ul64_sub(_dst, _dst, n->n);
    #endif
}

/*
 * Convert a ul64 into Montgomery form
 */
inline void ul64_to_montgomery(ul64 dst, ul64 src, mod64 mod) {
    ul64_modmul(dst, src, mod->rsq, mod);
}

/*
 * Convert a ul64 out-of Montgomery form
 */
inline void ul64_from_montgomery(ul64 dst, ul64 src, mod64 mod) {
    ul64 one = { 0 };
    one->x[0] = 1;
    
    ul64_modmul(dst, src, one, mod);
}




/*
 * Right-shift a ul64 by some number of bits
 */
inline void ul64_rshift(ul64 dst, ul64 src, int shift) {
dst->x[0] = (src->x[0] >> shift) | (src->x[1] << (32 - shift));
dst->x[1] = dst->x[1] >> shift;
}



/*
 * Left shift a ul64 by some number of words
 */
inline void ul64_lshiftw(ul64 dst, ul64 src, int w) {
    dst->x[1] = ((1-w) >= 0) ? src->x[1-w] : 0;
    dst->x[0] = ((0-w) >= 0) ? src->x[0-w] : 0;
}

/*
 * Multiply a ul64 by a uint32_t
 */
inline void ul64_mulu32(ul64 dst, ul64 src, uint32_t x) {
    uint64_t x_src_0 = ((uint64_t)src->x[0]) * ((uint64_t)x);
    uint64_t x_src_1 = ((uint64_t)src->x[1]) * ((uint64_t)x);
    
    dst->x[0] = 0;
    dst->x[1] = 0;
    
    *(uint64_t*)(&dst->x[0]) += x_src_0;
    dst->x[1] += x_src_1 >> 32;
}




#endif
