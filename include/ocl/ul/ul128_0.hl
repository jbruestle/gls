/*
 * AUTOGENERATED tcarstens January 2014
 */
#ifndef __UL128_0__
#define __UL128_0__

#include "../stdint.hl"
#include "../rng.hl"


/*
 * ul128
 */
typedef struct ul128_s {
    uint32_t x[4];
} ul128[1];

inline void ul128_init(ul128 x) { return; }
inline void ul128_clear(ul128 x) { return; }

typedef struct mod128_s {
    ul128 n;
    uint32_t np;
    ul128 rsq;
} mod128[1];




/*
 * Setters
 */
inline void ul128_set_gp(__global struct ul128_s *dst, ul128 src) {
    dst->x[0] = src->x[0];
    dst->x[1] = src->x[1];
    dst->x[2] = src->x[2];
    dst->x[3] = src->x[3];
}
inline void ul128_set_pg(ul128 dst, __global struct ul128_s *src) {
    dst->x[0] = src->x[0];
    dst->x[1] = src->x[1];
    dst->x[2] = src->x[2];
    dst->x[3] = src->x[3];
}
inline void ul128_set_gg(__global struct ul128_s *dst, __global struct ul128_s *src) {
    dst->x[0] = src->x[0];
    dst->x[1] = src->x[1];
    dst->x[2] = src->x[2];
    dst->x[3] = src->x[3];
}
inline void ul128_set(ul128 dst, ul128 src) {
    dst->x[0] = src->x[0];
    dst->x[1] = src->x[1];
    dst->x[2] = src->x[2];
    dst->x[3] = src->x[3];
}
/*
 * Set a ul128 to a uint32_t
 */
inline void ul128_set_ui(ul128 dst, uint32_t i) {
    dst->x[0] = i;
    dst->x[1] = 0;
    dst->x[2] = 0;
    dst->x[3] = 0;
}

/*
 * Get a uint32_t out of a ul128
 */
inline uint32_t ul128_get_ui(ul128 src) {
    return src->x[0];
}




/*
 * Generate a random ul128
 */
inline void ul128_rand(struct rng_t *r, ul128 dst) {
    uint64_t w0 = rand_uint64(r);
    uint64_t w1 = rand_uint64(r);
    
    dst->x[0] = w0 & 0xffffffff;
    dst->x[1] = w0  >> 32;
    dst->x[2] = w1 & 0xffffffff;
    dst->x[3] = w1  >> 32;
}



/*
 * Compare two ul128's
 */
inline int ul128_cmp(ul128 src1, ul128 src2) {
    int r = 0;
    if (src1->x[3] > src2->x[3]) r = 1;
    else if (src1->x[3] < src2->x[3]) r = -1;
    else if (src1->x[2] > src2->x[2]) r = 1;
    else if (src1->x[2] < src2->x[2]) r = -1;
    else if (src1->x[1] > src2->x[1]) r = 1;
    else if (src1->x[1] < src2->x[1]) r = -1;
    else if (src1->x[0] > src2->x[0]) r = 1;
    else if (src1->x[0] < src2->x[0]) r = -1;
    return r;
}

/*
 * Compare a ul128 with a uint32_t
 */
inline int ul128_cmp_ui(ul128 src1, uint32_t src2) {
    int r = 0;
    if (src1->x[3] | src1->x[2] |  src1->x[1]) r = 1;
    else if (src1->x[0] > src2) r = 1;
    else if (src1->x[0] < src2) r = -1;
    return r;
}




/*
 * Add two ul128's
 */
inline void ul128_add(ul128 dst, ul128 src1, ul128 src2) {
    #if defined(UL_NVIDIA)
        asm(
          "add.cc.u32  %0, %4, %8;\n\t"
          "addc.cc.u32 %1, %5, %9;\n\t"
          "addc.cc.u32 %2, %6, %10;\n\t"
          "addc.u32    %3, %7, %11;\n\t"
          : "=r" (dst->x[0]), "=r" (dst->x[1]), "=r" (dst->x[2]), "=r" (dst->x[3])
          : "r" (src1->x[0]), "r" (src1->x[1]), "r" (src1->x[2]), "r" (src1->x[3]), 
            "r" (src2->x[0]), "r" (src2->x[1]), "r" (src2->x[2]), "r" (src2->x[3])
          : "cc"
        );
    #else
        ul128 d = {{{0}}};
        d->x[0] = (src1->x[0] & 0x7fffffff) + (src2->x[0] & 0x7fffffff) + 0;
        uint32_t c0 = (src1->x[0] >> 31) + (src2->x[0] >> 31) + (d->x[0] >> 31);
        d->x[0] = (c0 << 31) | (d->x[0] & 0x7fffffff);
        c0 = c0 >> 1;
        dst->x[0] = d->x[0];
        
        d->x[1] = (src1->x[1] & 0x7fffffff) + (src2->x[1] & 0x7fffffff) + c0;
        uint32_t c1 = (src1->x[1] >> 31) + (src2->x[1] >> 31) + (d->x[1] >> 31);
        d->x[1] = (c1 << 31) | (d->x[1] & 0x7fffffff);
        c1 = c1 >> 1;
        dst->x[1] = d->x[1];
        
        d->x[2] = (src1->x[2] & 0x7fffffff) + (src2->x[2] & 0x7fffffff) + c1;
        uint32_t c2 = (src1->x[2] >> 31) + (src2->x[2] >> 31) + (d->x[2] >> 31);
        d->x[2] = (c2 << 31) | (d->x[2] & 0x7fffffff);
        c2 = c2 >> 1;
        dst->x[2] = d->x[2];
        
        d->x[3] = (src1->x[3] & 0x7fffffff) + (src2->x[3] & 0x7fffffff) + c2;
        uint32_t c3 = (src1->x[3] >> 31) + (src2->x[3] >> 31) + (d->x[3] >> 31);
        d->x[3] = (c3 << 31) | (d->x[3] & 0x7fffffff);
        c3 = c3 >> 1;
        dst->x[3] = d->x[3];
        
    #endif
    return;
}
/*
 * Sub two ul128's
 */
inline void ul128_sub(ul128 dst, ul128 src1, ul128 src2) {
    #if defined(UL_NVIDIA)
        asm(
          "sub.cc.u32  %0, %4, %8;\n\t"
          "subc.cc.u32 %1, %5, %9;\n\t"
          "subc.cc.u32 %2, %6, %10;\n\t"
          "subc.u32    %3, %7, %11;\n\t"
          : "=r" (dst->x[0]), "=r" (dst->x[1]), "=r" (dst->x[2]), "=r" (dst->x[3])
          : "r" (src1->x[0]), "r" (src1->x[1]), "r" (src1->x[2]), "r" (src1->x[3]), 
            "r" (src2->x[0]), "r" (src2->x[1]), "r" (src2->x[2]), "r" (src2->x[3])
          : "cc"
        );
    #else
        ul128 d = {{{0}}};
        d->x[0] = (src1->x[0] & 0x7fffffff) - (src2->x[0] & 0x7fffffff) - 0;
        uint32_t b0 = (src1->x[0] >> 31) - (src2->x[0] >> 31) - (d->x[0] >> 31);
        d->x[0] = (b0 << 31) | (d->x[0] & 0x7fffffff);
        b0 = (b0 >> 1) & 1;
        dst->x[0] = d->x[0];
        
        d->x[1] = (src1->x[1] & 0x7fffffff) - (src2->x[1] & 0x7fffffff) - b0;
        uint32_t b1 = (src1->x[1] >> 31) - (src2->x[1] >> 31) - (d->x[1] >> 31);
        d->x[1] = (b1 << 31) | (d->x[1] & 0x7fffffff);
        b1 = (b1 >> 1) & 1;
        dst->x[1] = d->x[1];
        
        d->x[2] = (src1->x[2] & 0x7fffffff) - (src2->x[2] & 0x7fffffff) - b1;
        uint32_t b2 = (src1->x[2] >> 31) - (src2->x[2] >> 31) - (d->x[2] >> 31);
        d->x[2] = (b2 << 31) | (d->x[2] & 0x7fffffff);
        b2 = (b2 >> 1) & 1;
        dst->x[2] = d->x[2];
        
        d->x[3] = (src1->x[3] & 0x7fffffff) - (src2->x[3] & 0x7fffffff) - b2;
        uint32_t b3 = (src1->x[3] >> 31) - (src2->x[3] >> 31) - (d->x[3] >> 31);
        d->x[3] = (b3 << 31) | (d->x[3] & 0x7fffffff);
        b3 = (b3 >> 1) & 1;
        dst->x[3] = d->x[3];
        
    #endif
    return;
}
/*
 * Mul two ul128's
 */
inline void ul128_mul(ul128 dst, ul128 src1, ul128 src2) {
    #if defined(UL_NVIDIA)
        asm(
          "mul.lo.u32 %0, %4, %8;\n\t"
          "mul.hi.u32 %1, %4, %8;\n\t"
          "mad.lo.cc.u32 %1, %4, %9, %1;\n\t"
          "madc.hi.u32 %2, %4, %9, 0;\n\t"
          "mad.lo.cc.u32 %1, %5, %8, %1;\n\t"
          "madc.hi.cc.u32 %2, %5, %8, %2;\n\t"
          "madc.lo.u32 %3, %4, %11, 0;\n\t"
          "mad.lo.cc.u32 %2, %4, %10, %2;\n\t"
          "madc.hi.u32 %3, %4, %10, %3;\n\t"
          "mad.lo.cc.u32 %2, %5, %9, %2;\n\t"
          "madc.hi.u32 %3, %5, %9, %3;\n\t"
          "mad.lo.cc.u32 %2, %6, %8, %2;\n\t"
          "madc.hi.u32 %3, %6, %8, %3;\n\t"
          "mad.lo.u32 %3, %5, %10, %3;\n\t"
          "mad.lo.u32 %3, %6, %9, %3;\n\t"
          "mad.lo.u32 %3, %7, %8, %3;\n\t"
          : "=r" (dst->x[0]), "=r" (dst->x[1]), "=r" (dst->x[2]), "=r" (dst->x[3])
          : "r" (src1->x[0]), "r" (src1->x[1]), "r" (src1->x[2]), "r" (src1->x[3]), 
            "r" (src2->x[0]), "r" (src2->x[1]), "r" (src2->x[2]), "r" (src2->x[3])
          : "cc"
        );
    #else
        ul128 d = {{{0}}};
        
        uint64_t tmp0 = ((uint64_t)src1->x[0]) * ((uint64_t)src2->x[0]);
        ul128 tmp0_;
        tmp0_->x[0] = tmp0;
        tmp0_->x[1] = tmp0 >> 32;
        tmp0_->x[2] = 0;
        tmp0_->x[3] = 0;
        ul128_add(d, tmp0_, d);
        
        uint64_t tmp1 = ((uint64_t)src1->x[0]) * ((uint64_t)src2->x[1]);
        ul128 tmp1_;
        tmp1_->x[0] = 0;
        tmp1_->x[1] = tmp1;
        tmp1_->x[2] = tmp1 >> 32;
        tmp1_->x[3] = 0;
        ul128_add(d, tmp1_, d);
        
        uint64_t tmp2 = ((uint64_t)src1->x[1]) * ((uint64_t)src2->x[0]);
        ul128 tmp2_;
        tmp2_->x[0] = 0;
        tmp2_->x[1] = tmp2;
        tmp2_->x[2] = tmp2 >> 32;
        tmp2_->x[3] = 0;
        ul128_add(d, tmp2_, d);
        
        uint64_t tmp3 = ((uint64_t)src1->x[0]) * ((uint64_t)src2->x[2]);
        ul128 tmp3_;
        tmp3_->x[0] = 0;
        tmp3_->x[1] = 0;
        tmp3_->x[2] = tmp3;
        tmp3_->x[3] = tmp3 >> 32;
        ul128_add(d, tmp3_, d);
        
        uint64_t tmp4 = ((uint64_t)src1->x[1]) * ((uint64_t)src2->x[1]);
        ul128 tmp4_;
        tmp4_->x[0] = 0;
        tmp4_->x[1] = 0;
        tmp4_->x[2] = tmp4;
        tmp4_->x[3] = tmp4 >> 32;
        ul128_add(d, tmp4_, d);
        
        uint64_t tmp5 = ((uint64_t)src1->x[2]) * ((uint64_t)src2->x[0]);
        ul128 tmp5_;
        tmp5_->x[0] = 0;
        tmp5_->x[1] = 0;
        tmp5_->x[2] = tmp5;
        tmp5_->x[3] = tmp5 >> 32;
        ul128_add(d, tmp5_, d);
        
        uint64_t tmp6 = ((uint64_t)src1->x[0]) * ((uint64_t)src2->x[3]);
        ul128 tmp6_;
        tmp6_->x[0] = 0;
        tmp6_->x[1] = 0;
        tmp6_->x[2] = 0;
        tmp6_->x[3] = tmp6;
        ul128_add(d, tmp6_, d);
        
        uint64_t tmp7 = ((uint64_t)src1->x[1]) * ((uint64_t)src2->x[2]);
        ul128 tmp7_;
        tmp7_->x[0] = 0;
        tmp7_->x[1] = 0;
        tmp7_->x[2] = 0;
        tmp7_->x[3] = tmp7;
        ul128_add(d, tmp7_, d);
        
        uint64_t tmp8 = ((uint64_t)src1->x[2]) * ((uint64_t)src2->x[1]);
        ul128 tmp8_;
        tmp8_->x[0] = 0;
        tmp8_->x[1] = 0;
        tmp8_->x[2] = 0;
        tmp8_->x[3] = tmp8;
        ul128_add(d, tmp8_, d);
        
        uint64_t tmp9 = ((uint64_t)src1->x[3]) * ((uint64_t)src2->x[0]);
        ul128 tmp9_;
        tmp9_->x[0] = 0;
        tmp9_->x[1] = 0;
        tmp9_->x[2] = 0;
        tmp9_->x[3] = tmp9;
        ul128_add(d, tmp9_, d);
        
        dst->x[0] = d->x[0];
        dst->x[1] = d->x[1];
        dst->x[2] = d->x[2];
        dst->x[3] = d->x[3];
    #endif
    return;
}




/*
 * Initialize mod128
 */
inline void mod128_init(mod128 n) {
}

/*
 * Add two ul128's modulo another
 */
inline void ul128_modadd(ul128 dst, ul128 src1, ul128 src2, mod128 n) {
    ul128_add(dst, src1, src2);
    if (ul128_cmp(dst, n->n) >= 0)
        ul128_sub(dst, dst, n->n);
}

/*
 * Subtract one ul128 from another modulo a third
 */
inline void ul128_modsub(ul128 dst, ul128 src1, ul128 src2, mod128 n) {
    ul128 tr1, tr2;
    ul128_sub(tr1, src1, src2);
    ul128_add(tr2, tr1, n->n);
    if (ul128_cmp(src1, src2) >= 0)
        ul128_set(dst, tr1);
    else
        ul128_set(dst, tr2);
}

/*
 * Mul two ul128's modulo a third, followed by Montgomery reduction
 */
void ul128_modmul(ul128 _dst, ul128 _src1, ul128 _src2, mod128 n);
void ul128_modmul(ul128 _dst, ul128 _src1, ul128 _src2, mod128 n) {
    #if defined(UL_NVIDIA)
        volatile ul128 src1;
        volatile ul128 src2;
        /* ul128_set(src1, _src1); */
        src1->x[0] = _src1->x[0];
        src1->x[1] = _src1->x[1];
        src1->x[2] = _src1->x[2];
        src1->x[3] = _src1->x[3];
        /* ul128_set(src2, _src2); */
        src2->x[0] = _src2->x[0];
        src2->x[1] = _src2->x[1];
        src2->x[2] = _src2->x[2];
        src2->x[3] = _src2->x[3];
        
        uint32_t q = 0;
        ul128 dst = {{{0}}};
        uint32_t dst_4 = 0;
        
        asm(
          /* Compute c_0..3 for the product a*b, with carry-out to dst_4 */
          "mad.lo.cc.u32  %0, %6, %10, %0;\n\t"  /* c_0 += lo(a_0, b_0) */
          "madc.hi.cc.u32 %1, %6, %10, %1;\n\t"  /* c_1 += hi(a_0, b_0) */
          "madc.lo.cc.u32 %2, %6, %12, %2;\n\t"  /* c_2 += lo(a_0, b_2) */
          "madc.hi.cc.u32 %3, %6, %12, %3;\n\t"  /* c_3 += hi(a_0, b_2) */
          "addc.u32 %4, %4, 0;\n\t"  /* accum carry in c_4 */
          "mad.lo.cc.u32  %1, %6, %11, %1;\n\t"  /* c_1 += lo(a_0, b_1) */
          "madc.hi.cc.u32 %2, %6, %11, %2;\n\t"  /* c_2 += hi(a_0, b_1) */
          "madc.lo.cc.u32 %3, %6, %13, %3;\n\t"  /* c_3 += lo(a_0, b_3) */
          "addc.u32 %4, %4, 0;\n\t"  /* accum carry in c_4 */
          "mad.lo.cc.u32  %1, %7, %10, %1;\n\t"  /* c_1 += lo(a_1, b_0) */
          "madc.hi.cc.u32 %2, %7, %10, %2;\n\t"  /* c_2 += hi(a_1, b_0) */
          "madc.lo.cc.u32 %3, %7, %12, %3;\n\t"  /* c_3 += lo(a_1, b_2) */
          "addc.u32 %4, %4, 0;\n\t"  /* accum carry in c_4 */
          "mad.lo.cc.u32  %2, %7, %11, %2;\n\t"  /* c_2 += lo(a_1, b_1) */
          "madc.hi.cc.u32 %3, %7, %11, %3;\n\t"  /* c_3 += hi(a_1, b_1) */
          "addc.u32 %4, %4, 0;\n\t"  /* accum carry in c_4 */
          "mad.lo.cc.u32  %2, %8, %10, %2;\n\t"  /* c_2 += lo(a_2, b_0) */
          "madc.hi.cc.u32 %3, %8, %10, %3;\n\t"  /* c_3 += hi(a_2, b_0) */
          "addc.u32 %4, %4, 0;\n\t"  /* accum carry in c_4 */
          "mad.lo.cc.u32  %3, %8, %11, %3;\n\t"  /* c_3 += lo(a_2, b_1) */
          "addc.u32 %4, %4, 0;\n\t"  /* accum carry in c_4 */
          "mad.lo.cc.u32  %3, %9, %10, %3;\n\t"  /* c_3 += lo(a_3, b_0) */
          "addc.u32 %4, %4, 0;\n\t"  /* accum carry in c_4 */
          
          /* Add in the qN's */
          /* n = 0... */
            /* Compute q = mu * c_0 */
            "mov.u32 %5, %0;\n\t"
            "mul.lo.u32 %5, %5, %18;\n\t"
            /* Update c_0 with qN_0 */
            "mad.lo.cc.u32 %0, %5, %14, %0;\n\t"  /* c_0 += lo(q, n_0) */
            /* Shift */
            "mov.u32 %0, %1;\n\t"  /* dst_0 <- c_1 */
            "mov.u32 %1, %2;\n\t"  /* dst_1 <- c_2 */
            "mov.u32 %2, %3;\n\t"  /* dst_2 <- c_3 */
            "mov.u32 %3, %4;\n\t"  /* dst_3 <- c_4 */
            "xor.b32 %4, %4, %4;\n\t"  /* dst_4 <- c_5 */
            /* Compute and add-in qN, with carry-out to dst_4 */
            "madc.hi.cc.u32 %0, %5, %14, %0;\n\t"  /* c_1 += hi(q, n_0) */
            "madc.lo.cc.u32 %1, %5, %16, %1;\n\t"  /* c_2 += lo(q, n_2) */
            "madc.hi.cc.u32 %2, %5, %16, %2;\n\t"  /* c_3 += hi(q, n_2) */
            "addc.cc.u32 %3, %3, 0;\n\t"  /* accum carry in dst_3 */
            "addc.u32    %4, %4, 0;\n\t"  /* accum carry in dst_4 */
            "mad.lo.cc.u32  %0, %5, %15, %0;\n\t"  /* c_1 += lo(q, n_1) */
            "madc.hi.cc.u32 %1, %5, %15, %1;\n\t"  /* c_2 += hi(q, n_1) */
            "madc.lo.cc.u32 %2, %5, %17, %2;\n\t"  /* c_3 += lo(q, n_3) */
            "madc.hi.cc.u32 %3, %5, %17, %3;\n\t"  /* c_4 += hi(q, n_3) */
            "addc.u32    %4, %4, 0;\n\t"  /* accum carry in dst_4 */
          /* n = 1... */
            /* Compute q = mu * c_1 */
            "mov.u32 %5, %0;\n\t"
            "mul.lo.u32 %5, %5, %18;\n\t"
            /* Update c_1 with qN_0 */
            "mad.lo.cc.u32 %0, %5, %14, %0;\n\t"  /* c_1 += lo(q, n_0) */
            /* Shift */
            "mov.u32 %0, %1;\n\t"  /* dst_0 <- c_2 */
            "mov.u32 %1, %2;\n\t"  /* dst_1 <- c_3 */
            "mov.u32 %2, %3;\n\t"  /* dst_2 <- c_4 */
            "mov.u32 %3, %4;\n\t"  /* dst_3 <- c_5 */
            "xor.b32 %4, %4, %4;\n\t"  /* dst_4 <- c_6 */
            /* Compute and add-in qN, with carry-out to dst_4 */
            "madc.hi.cc.u32 %0, %5, %14, %0;\n\t"  /* c_2 += hi(q, n_0) */
            "madc.lo.cc.u32 %1, %5, %16, %1;\n\t"  /* c_3 += lo(q, n_2) */
            "madc.hi.cc.u32 %2, %5, %16, %2;\n\t"  /* c_4 += hi(q, n_2) */
            "addc.cc.u32 %3, %3, 0;\n\t"  /* accum carry in dst_3 */
            "addc.u32    %4, %4, 0;\n\t"  /* accum carry in dst_4 */
            "mad.lo.cc.u32  %0, %5, %15, %0;\n\t"  /* c_1 += lo(q, n_1) */
            "madc.hi.cc.u32 %1, %5, %15, %1;\n\t"  /* c_2 += hi(q, n_1) */
            "madc.lo.cc.u32 %2, %5, %17, %2;\n\t"  /* c_3 += lo(q, n_3) */
            "madc.hi.cc.u32 %3, %5, %17, %3;\n\t"  /* c_4 += hi(q, n_3) */
            "addc.u32    %4, %4, 0;\n\t"  /* accum carry in dst_4 */
          /* n = 2... */
            /* Compute q = mu * c_2 */
            "mov.u32 %5, %0;\n\t"
            "mul.lo.u32 %5, %5, %18;\n\t"
            /* Update c_2 with qN_0 */
            "mad.lo.cc.u32 %0, %5, %14, %0;\n\t"  /* c_2 += lo(q, n_0) */
            /* Shift */
            "mov.u32 %0, %1;\n\t"  /* dst_0 <- c_3 */
            "mov.u32 %1, %2;\n\t"  /* dst_1 <- c_4 */
            "mov.u32 %2, %3;\n\t"  /* dst_2 <- c_5 */
            "mov.u32 %3, %4;\n\t"  /* dst_3 <- c_6 */
            "xor.b32 %4, %4, %4;\n\t"  /* dst_4 <- c_7 */
            /* Compute and add-in qN, with carry-out to dst_4 */
            "madc.hi.cc.u32 %0, %5, %14, %0;\n\t"  /* c_3 += hi(q, n_0) */
            "madc.lo.cc.u32 %1, %5, %16, %1;\n\t"  /* c_4 += lo(q, n_2) */
            "madc.hi.cc.u32 %2, %5, %16, %2;\n\t"  /* c_5 += hi(q, n_2) */
            "addc.cc.u32 %3, %3, 0;\n\t"  /* accum carry in dst_3 */
            "addc.u32    %4, %4, 0;\n\t"  /* accum carry in dst_4 */
            "mad.lo.cc.u32  %0, %5, %15, %0;\n\t"  /* c_1 += lo(q, n_1) */
            "madc.hi.cc.u32 %1, %5, %15, %1;\n\t"  /* c_2 += hi(q, n_1) */
            "madc.lo.cc.u32 %2, %5, %17, %2;\n\t"  /* c_3 += lo(q, n_3) */
            "madc.hi.cc.u32 %3, %5, %17, %3;\n\t"  /* c_4 += hi(q, n_3) */
            "addc.u32    %4, %4, 0;\n\t"  /* accum carry in dst_4 */
          /* n = 3... */
            /* Compute q = mu * c_3 */
            "mov.u32 %5, %0;\n\t"
            "mul.lo.u32 %5, %5, %18;\n\t"
            /* Update c_3 with qN_0 */
            "mad.lo.cc.u32 %0, %5, %14, %0;\n\t"  /* c_3 += lo(q, n_0) */
            /* Shift */
            "mov.u32 %0, %1;\n\t"  /* dst_0 <- c_4 */
            "mov.u32 %1, %2;\n\t"  /* dst_1 <- c_5 */
            "mov.u32 %2, %3;\n\t"  /* dst_2 <- c_6 */
            "mov.u32 %3, %4;\n\t"  /* dst_3 <- c_7 */
            "xor.b32 %4, %4, %4;\n\t"  /* dst_4 <- c_8 */
            /* Compute and add-in qN, with carry-out to dst_4 */
            "madc.hi.cc.u32 %0, %5, %14, %0;\n\t"  /* c_4 += hi(q, n_0) */
            "madc.lo.cc.u32 %1, %5, %16, %1;\n\t"  /* c_5 += lo(q, n_2) */
            "madc.hi.cc.u32 %2, %5, %16, %2;\n\t"  /* c_6 += hi(q, n_2) */
            "addc.cc.u32 %3, %3, 0;\n\t"  /* accum carry in dst_3 */
            "addc.u32    %4, %4, 0;\n\t"  /* accum carry in dst_4 */
            "mad.lo.cc.u32  %0, %5, %15, %0;\n\t"  /* c_1 += lo(q, n_1) */
            "madc.hi.cc.u32 %1, %5, %15, %1;\n\t"  /* c_2 += hi(q, n_1) */
            "madc.lo.cc.u32 %2, %5, %17, %2;\n\t"  /* c_3 += lo(q, n_3) */
            "madc.hi.cc.u32 %3, %5, %17, %3;\n\t"  /* c_4 += hi(q, n_3) */
            "addc.u32    %4, %4, 0;\n\t"  /* accum carry in dst_4 */
          
          /* Compute c_4..7 in the product a*b, storing the result in dst_0..3, with carry-out to dst_4 */
          "mad.hi.cc.u32  %0, %6, %13, %0;\n\t"  /* c_4 += hi(a_0, b_3) */
          "addc.cc.u32 %1, %1, 0;\n\t"  /* accum carry in c_5 */
          "addc.cc.u32 %2, %2, 0;\n\t"  /* accum carry in c_6 */
          "addc.cc.u32 %3, %3, 0;\n\t"  /* accum carry in c_7 */
          "addc.u32    %4, %4, 0;\n\t"  /* accum carry in c_8 */
          "mad.hi.cc.u32  %0, %7, %12, %0;\n\t"  /* c_4 += hi(a_1, b_2) */
          "addc.cc.u32 %1, %1, 0;\n\t"  /* accum carry in c_5 */
          "addc.cc.u32 %2, %2, 0;\n\t"  /* accum carry in c_6 */
          "addc.cc.u32 %3, %3, 0;\n\t"  /* accum carry in c_7 */
          "addc.u32    %4, %4, 0;\n\t"  /* accum carry in c_8 */
          "mad.lo.cc.u32  %0, %7, %13, %0;\n\t"  /* c_4 += lo(a_1, b_3) */
          "madc.hi.cc.u32 %1, %7, %13, %1;\n\t"  /* c_5 += hi(a_1, b_3) */
          "addc.cc.u32 %2, %2, 0;\n\t"  /* accum carry in c_6 */
          "addc.cc.u32 %3, %3, 0;\n\t"  /* accum carry in c_7 */
          "addc.u32    %4, %4, 0;\n\t"  /* accum carry in c_8 */
          "mad.lo.cc.u32  %0, %8, %12, %0;\n\t"  /* c_4 += lo(a_2, b_2) */
          "madc.hi.cc.u32 %1, %8, %12, %1;\n\t"  /* c_5 += hi(a_2, b_2) */
          "addc.cc.u32 %2, %2, 0;\n\t"  /* accum carry in c_6 */
          "addc.cc.u32 %3, %3, 0;\n\t"  /* accum carry in c_7 */
          "addc.u32    %4, %4, 0;\n\t"  /* accum carry in c_8 */
          "mad.hi.cc.u32  %0, %8, %11, %0;\n\t"  /* c_4 += hi(a_2, b_1) */
          "madc.lo.cc.u32 %1, %8, %13, %1;\n\t"  /* c_5 += lo(a_2, b_3) */
          "madc.hi.cc.u32 %2, %8, %13, %2;\n\t"  /* c_6 += hi(a_2, b_3) */
          "addc.cc.u32 %3, %3, 0;\n\t"  /* accum carry in c_7 */
          "addc.u32    %4, %4, 0;\n\t"  /* accum carry in c_8 */
          "mad.hi.cc.u32  %0, %9, %10, %0;\n\t"  /* c_4 += hi(a_3, b_0) */
          "madc.lo.cc.u32 %1, %9, %12, %1;\n\t"  /* c_5 += lo(a_3, b_2) */
          "madc.hi.cc.u32 %2, %9, %12, %2;\n\t"  /* c_6 += hi(a_3, b_2) */
          "addc.cc.u32 %3, %3, 0;\n\t"  /* accum carry in c_7 */
          "addc.u32    %4, %4, 0;\n\t"  /* accum carry in c_8 */
          "mad.lo.cc.u32  %0, %9, %11, %0;\n\t"  /* c_4 += lo(a_3, b_1) */
          "madc.hi.cc.u32 %1, %9, %11, %1;\n\t"  /* c_5 += hi(a_3, b_1) */
          "madc.lo.cc.u32 %2, %9, %13, %2;\n\t"  /* c_6 += lo(a_3, b_3) */
          "madc.hi.cc.u32 %3, %9, %13, %3;\n\t"  /* c_7 += hi(a_3, b_3) */
          "addc.u32    %4, %4, 0;\n\t"  /* accum carry in c_8 */
          
          : "+r" (dst->x[0]), "+r" (dst->x[1]), "+r" (dst->x[2]), "+r" (dst->x[3]), "+r" (dst_4), "+r" (q)
          : "r" (src1->x[0]), "r" (src1->x[1]), "r" (src1->x[2]), "r" (src1->x[3]), 
            "r" (src2->x[0]), "r" (src2->x[1]), "r" (src2->x[2]), "r" (src2->x[3]), 
            "r" (n->n->x[0]), "r" (n->n->x[1]), "r" (n->n->x[2]), "r" (n->n->x[3]), 
            "r" (n->np)
        );
        
        /* Reduce as needed */
        if (dst_4 || (ul128_cmp(dst, n->n) >= 0))
            ul128_sub(dst, dst, n->n);
        ul128_set(_dst, dst);
    #else
        /* The pairwise products of the a_i and b_j */
        const uint64_t a_0__b_0 = ((uint64_t)_src1->x[0]) * ((uint64_t)_src2->x[0]);
        const uint32_t a_0__b_0_lo = a_0__b_0;
        const uint32_t a_0__b_0_hi = a_0__b_0 >> 32;
        const uint64_t a_0__b_1 = ((uint64_t)_src1->x[0]) * ((uint64_t)_src2->x[1]);
        const uint32_t a_0__b_1_lo = a_0__b_1;
        const uint32_t a_0__b_1_hi = a_0__b_1 >> 32;
        const uint64_t a_0__b_2 = ((uint64_t)_src1->x[0]) * ((uint64_t)_src2->x[2]);
        const uint32_t a_0__b_2_lo = a_0__b_2;
        const uint32_t a_0__b_2_hi = a_0__b_2 >> 32;
        const uint64_t a_0__b_3 = ((uint64_t)_src1->x[0]) * ((uint64_t)_src2->x[3]);
        const uint32_t a_0__b_3_lo = a_0__b_3;
        const uint32_t a_0__b_3_hi = a_0__b_3 >> 32;
        const uint64_t a_1__b_0 = ((uint64_t)_src1->x[1]) * ((uint64_t)_src2->x[0]);
        const uint32_t a_1__b_0_lo = a_1__b_0;
        const uint32_t a_1__b_0_hi = a_1__b_0 >> 32;
        const uint64_t a_1__b_1 = ((uint64_t)_src1->x[1]) * ((uint64_t)_src2->x[1]);
        const uint32_t a_1__b_1_lo = a_1__b_1;
        const uint32_t a_1__b_1_hi = a_1__b_1 >> 32;
        const uint64_t a_1__b_2 = ((uint64_t)_src1->x[1]) * ((uint64_t)_src2->x[2]);
        const uint32_t a_1__b_2_lo = a_1__b_2;
        const uint32_t a_1__b_2_hi = a_1__b_2 >> 32;
        const uint64_t a_1__b_3 = ((uint64_t)_src1->x[1]) * ((uint64_t)_src2->x[3]);
        const uint32_t a_1__b_3_lo = a_1__b_3;
        const uint32_t a_1__b_3_hi = a_1__b_3 >> 32;
        const uint64_t a_2__b_0 = ((uint64_t)_src1->x[2]) * ((uint64_t)_src2->x[0]);
        const uint32_t a_2__b_0_lo = a_2__b_0;
        const uint32_t a_2__b_0_hi = a_2__b_0 >> 32;
        const uint64_t a_2__b_1 = ((uint64_t)_src1->x[2]) * ((uint64_t)_src2->x[1]);
        const uint32_t a_2__b_1_lo = a_2__b_1;
        const uint32_t a_2__b_1_hi = a_2__b_1 >> 32;
        const uint64_t a_2__b_2 = ((uint64_t)_src1->x[2]) * ((uint64_t)_src2->x[2]);
        const uint32_t a_2__b_2_lo = a_2__b_2;
        const uint32_t a_2__b_2_hi = a_2__b_2 >> 32;
        const uint64_t a_2__b_3 = ((uint64_t)_src1->x[2]) * ((uint64_t)_src2->x[3]);
        const uint32_t a_2__b_3_lo = a_2__b_3;
        const uint32_t a_2__b_3_hi = a_2__b_3 >> 32;
        const uint64_t a_3__b_0 = ((uint64_t)_src1->x[3]) * ((uint64_t)_src2->x[0]);
        const uint32_t a_3__b_0_lo = a_3__b_0;
        const uint32_t a_3__b_0_hi = a_3__b_0 >> 32;
        const uint64_t a_3__b_1 = ((uint64_t)_src1->x[3]) * ((uint64_t)_src2->x[1]);
        const uint32_t a_3__b_1_lo = a_3__b_1;
        const uint32_t a_3__b_1_hi = a_3__b_1 >> 32;
        const uint64_t a_3__b_2 = ((uint64_t)_src1->x[3]) * ((uint64_t)_src2->x[2]);
        const uint32_t a_3__b_2_lo = a_3__b_2;
        const uint32_t a_3__b_2_hi = a_3__b_2 >> 32;
        const uint64_t a_3__b_3 = ((uint64_t)_src1->x[3]) * ((uint64_t)_src2->x[3]);
        const uint32_t a_3__b_3_lo = a_3__b_3;
        const uint32_t a_3__b_3_hi = a_3__b_3 >> 32;
        
        /* Limbs of the product C = A*B */
        uint32_t q_0 = 0;
        uint32_t q_1 = 0;
        uint32_t q_2 = 0;
        uint32_t q_3 = 0;
        uint32_t c_0 = 0;
        uint32_t c_1 = 0;
        uint32_t c_2 = 0;
        uint32_t c_3 = 0;
        uint32_t c_4 = 0;
        uint32_t c_5 = 0;
        uint32_t c_6 = 0;
        uint32_t c_7 = 0;
        uint32_t c_8 = 0;
        
        /* The product of the q_i's with N */
        uint64_t q_0__N_0 = 0;
        uint64_t q_0__N_1 = 0;
        uint64_t q_0__N_2 = 0;
        uint64_t q_0__N_3 = 0;
        uint64_t q_1__N_0 = 0;
        uint64_t q_1__N_1 = 0;
        uint64_t q_1__N_2 = 0;
        uint64_t q_1__N_3 = 0;
        uint64_t q_2__N_0 = 0;
        uint64_t q_2__N_1 = 0;
        uint64_t q_2__N_2 = 0;
        uint64_t q_2__N_3 = 0;
        uint64_t q_3__N_0 = 0;
        uint64_t q_3__N_1 = 0;
        uint64_t q_3__N_2 = 0;
        uint64_t q_3__N_3 = 0;
        
        /* Compute c_0 */
        {
            /* Add the product terms into c_0, accumulating carries in c_1 */
            c_1 += ul32_addc(&c_0, &c_0, &a_0__b_0_lo);
            
            /* Add the q_i*N's for i < 0 */
            
            /* Compute q_0 and add its product with N */
            q_0 = n->np * c_0;
            q_0__N_0 = ((uint64_t)q_0) * ((uint64_t)n->n->x[0]);
            q_0__N_1 = ((uint64_t)q_0) * ((uint64_t)n->n->x[1]);
            q_0__N_2 = ((uint64_t)q_0) * ((uint64_t)n->n->x[2]);
            q_0__N_3 = ((uint64_t)q_0) * ((uint64_t)n->n->x[3]);
            const uint32_t q_0__N_0_lo = q_0__N_0;
            c_1 += ul32_addc(&c_0, &c_0, &q_0__N_0_lo);
        }
        
        /* Compute c_1 */
        {
            /* Add the product terms into c_1, accumulating carries in c_2 */
            c_2 += ul32_addc(&c_1, &c_1, &a_0__b_0_hi);
            c_2 += ul32_addc(&c_1, &c_1, &a_0__b_1_lo);
            c_2 += ul32_addc(&c_1, &c_1, &a_1__b_0_lo);
            
            /* Add the q_i*N's for i < 1 */
            const uint32_t q_0__N_1_lo = q_0__N_1;
            c_2 += ul32_addc(&c_1, &c_1, &q_0__N_1_lo);
            const uint32_t q_0__N_0_hi = q_0__N_0 >> 32;
            c_2 += ul32_addc(&c_1, &c_1, &q_0__N_0_hi);
            
            /* Compute q_1 and add its product with N */
            q_1 = n->np * c_1;
            q_1__N_0 = ((uint64_t)q_1) * ((uint64_t)n->n->x[0]);
            q_1__N_1 = ((uint64_t)q_1) * ((uint64_t)n->n->x[1]);
            q_1__N_2 = ((uint64_t)q_1) * ((uint64_t)n->n->x[2]);
            q_1__N_3 = ((uint64_t)q_1) * ((uint64_t)n->n->x[3]);
            const uint32_t q_1__N_0_lo = q_1__N_0;
            c_2 += ul32_addc(&c_1, &c_1, &q_1__N_0_lo);
        }
        
        /* Compute c_2 */
        {
            /* Add the product terms into c_2, accumulating carries in c_3 */
            c_3 += ul32_addc(&c_2, &c_2, &a_0__b_1_hi);
            c_3 += ul32_addc(&c_2, &c_2, &a_0__b_2_lo);
            c_3 += ul32_addc(&c_2, &c_2, &a_1__b_0_hi);
            c_3 += ul32_addc(&c_2, &c_2, &a_1__b_1_lo);
            c_3 += ul32_addc(&c_2, &c_2, &a_2__b_0_lo);
            
            /* Add the q_i*N's for i < 2 */
            const uint32_t q_0__N_2_lo = q_0__N_2;
            c_3 += ul32_addc(&c_2, &c_2, &q_0__N_2_lo);
            const uint32_t q_0__N_1_hi = q_0__N_1 >> 32;
            c_3 += ul32_addc(&c_2, &c_2, &q_0__N_1_hi);
            const uint32_t q_1__N_1_lo = q_1__N_1;
            c_3 += ul32_addc(&c_2, &c_2, &q_1__N_1_lo);
            const uint32_t q_1__N_0_hi = q_1__N_0 >> 32;
            c_3 += ul32_addc(&c_2, &c_2, &q_1__N_0_hi);
            
            /* Compute q_2 and add its product with N */
            q_2 = n->np * c_2;
            q_2__N_0 = ((uint64_t)q_2) * ((uint64_t)n->n->x[0]);
            q_2__N_1 = ((uint64_t)q_2) * ((uint64_t)n->n->x[1]);
            q_2__N_2 = ((uint64_t)q_2) * ((uint64_t)n->n->x[2]);
            q_2__N_3 = ((uint64_t)q_2) * ((uint64_t)n->n->x[3]);
            const uint32_t q_2__N_0_lo = q_2__N_0;
            c_3 += ul32_addc(&c_2, &c_2, &q_2__N_0_lo);
        }
        
        /* Compute c_3 */
        {
            /* Add the product terms into c_3, accumulating carries in c_4 */
            c_4 += ul32_addc(&c_3, &c_3, &a_0__b_2_hi);
            c_4 += ul32_addc(&c_3, &c_3, &a_0__b_3_lo);
            c_4 += ul32_addc(&c_3, &c_3, &a_1__b_1_hi);
            c_4 += ul32_addc(&c_3, &c_3, &a_1__b_2_lo);
            c_4 += ul32_addc(&c_3, &c_3, &a_2__b_0_hi);
            c_4 += ul32_addc(&c_3, &c_3, &a_2__b_1_lo);
            c_4 += ul32_addc(&c_3, &c_3, &a_3__b_0_lo);
            
            /* Add the q_i*N's for i < 3 */
            const uint32_t q_0__N_3_lo = q_0__N_3;
            c_4 += ul32_addc(&c_3, &c_3, &q_0__N_3_lo);
            const uint32_t q_0__N_2_hi = q_0__N_2 >> 32;
            c_4 += ul32_addc(&c_3, &c_3, &q_0__N_2_hi);
            const uint32_t q_1__N_2_lo = q_1__N_2;
            c_4 += ul32_addc(&c_3, &c_3, &q_1__N_2_lo);
            const uint32_t q_1__N_1_hi = q_1__N_1 >> 32;
            c_4 += ul32_addc(&c_3, &c_3, &q_1__N_1_hi);
            const uint32_t q_2__N_1_lo = q_2__N_1;
            c_4 += ul32_addc(&c_3, &c_3, &q_2__N_1_lo);
            const uint32_t q_2__N_0_hi = q_2__N_0 >> 32;
            c_4 += ul32_addc(&c_3, &c_3, &q_2__N_0_hi);
            
            /* Compute q_3 and add its product with N */
            q_3 = n->np * c_3;
            q_3__N_0 = ((uint64_t)q_3) * ((uint64_t)n->n->x[0]);
            q_3__N_1 = ((uint64_t)q_3) * ((uint64_t)n->n->x[1]);
            q_3__N_2 = ((uint64_t)q_3) * ((uint64_t)n->n->x[2]);
            q_3__N_3 = ((uint64_t)q_3) * ((uint64_t)n->n->x[3]);
            const uint32_t q_3__N_0_lo = q_3__N_0;
            c_4 += ul32_addc(&c_3, &c_3, &q_3__N_0_lo);
        }
        
        /* Compute c_4 */
        {
            /* Add the product terms into c_4, accumulating carries in c_5 */
            c_5 += ul32_addc(&c_4, &c_4, &a_0__b_3_hi);
            c_5 += ul32_addc(&c_4, &c_4, &a_1__b_2_hi);
            c_5 += ul32_addc(&c_4, &c_4, &a_1__b_3_lo);
            c_5 += ul32_addc(&c_4, &c_4, &a_2__b_1_hi);
            c_5 += ul32_addc(&c_4, &c_4, &a_2__b_2_lo);
            c_5 += ul32_addc(&c_4, &c_4, &a_3__b_0_hi);
            c_5 += ul32_addc(&c_4, &c_4, &a_3__b_1_lo);
            
            /* Add the q_i*N's for i < 4 */
            const uint32_t q_0__N_3_hi = q_0__N_3 >> 32;
            c_5 += ul32_addc(&c_4, &c_4, &q_0__N_3_hi);
            const uint32_t q_1__N_3_lo = q_1__N_3;
            c_5 += ul32_addc(&c_4, &c_4, &q_1__N_3_lo);
            const uint32_t q_1__N_2_hi = q_1__N_2 >> 32;
            c_5 += ul32_addc(&c_4, &c_4, &q_1__N_2_hi);
            const uint32_t q_2__N_2_lo = q_2__N_2;
            c_5 += ul32_addc(&c_4, &c_4, &q_2__N_2_lo);
            const uint32_t q_2__N_1_hi = q_2__N_1 >> 32;
            c_5 += ul32_addc(&c_4, &c_4, &q_2__N_1_hi);
            const uint32_t q_3__N_1_lo = q_3__N_1;
            c_5 += ul32_addc(&c_4, &c_4, &q_3__N_1_lo);
            const uint32_t q_3__N_0_hi = q_3__N_0 >> 32;
            c_5 += ul32_addc(&c_4, &c_4, &q_3__N_0_hi);
            
        }
        
        /* Compute c_5 */
        {
            /* Add the product terms into c_5, accumulating carries in c_6 */
            c_6 += ul32_addc(&c_5, &c_5, &a_1__b_3_hi);
            c_6 += ul32_addc(&c_5, &c_5, &a_2__b_2_hi);
            c_6 += ul32_addc(&c_5, &c_5, &a_2__b_3_lo);
            c_6 += ul32_addc(&c_5, &c_5, &a_3__b_1_hi);
            c_6 += ul32_addc(&c_5, &c_5, &a_3__b_2_lo);
            
            /* Add the q_i*N's for i < 5 */
            const uint32_t q_1__N_3_hi = q_1__N_3 >> 32;
            c_6 += ul32_addc(&c_5, &c_5, &q_1__N_3_hi);
            const uint32_t q_2__N_3_lo = q_2__N_3;
            c_6 += ul32_addc(&c_5, &c_5, &q_2__N_3_lo);
            const uint32_t q_2__N_2_hi = q_2__N_2 >> 32;
            c_6 += ul32_addc(&c_5, &c_5, &q_2__N_2_hi);
            const uint32_t q_3__N_2_lo = q_3__N_2;
            c_6 += ul32_addc(&c_5, &c_5, &q_3__N_2_lo);
            const uint32_t q_3__N_1_hi = q_3__N_1 >> 32;
            c_6 += ul32_addc(&c_5, &c_5, &q_3__N_1_hi);
            
        }
        
        /* Compute c_6 */
        {
            /* Add the product terms into c_6, accumulating carries in c_7 */
            c_7 += ul32_addc(&c_6, &c_6, &a_2__b_3_hi);
            c_7 += ul32_addc(&c_6, &c_6, &a_3__b_2_hi);
            c_7 += ul32_addc(&c_6, &c_6, &a_3__b_3_lo);
            
            /* Add the q_i*N's for i < 6 */
            const uint32_t q_2__N_3_hi = q_2__N_3 >> 32;
            c_7 += ul32_addc(&c_6, &c_6, &q_2__N_3_hi);
            const uint32_t q_3__N_3_lo = q_3__N_3;
            c_7 += ul32_addc(&c_6, &c_6, &q_3__N_3_lo);
            const uint32_t q_3__N_2_hi = q_3__N_2 >> 32;
            c_7 += ul32_addc(&c_6, &c_6, &q_3__N_2_hi);
            
        }
        
        /* Compute c_7 */
        {
            /* Add the product terms into c_7, accumulating carries in c_8 */
            c_8 += ul32_addc(&c_7, &c_7, &a_3__b_3_hi);
            
            /* Add the q_i*N's for i < 7 */
            const uint32_t q_3__N_3_hi = q_3__N_3 >> 32;
            c_8 += ul32_addc(&c_7, &c_7, &q_3__N_3_hi);
            
        }
        
        /* R = C * beta^{-n} */
        _dst->x[0] = c_4;
        _dst->x[1] = c_5;
        _dst->x[2] = c_6;
        _dst->x[3] = c_7;
        
        /* Reduce as needed */
        if (c_8 || (ul128_cmp(_dst, n->n) >= 0))
            ul128_sub(_dst, _dst, n->n);
    #endif
}

/*
 * Convert a ul128 into Montgomery form
 */
inline void ul128_to_montgomery(ul128 dst, ul128 src, mod128 mod) {
    ul128_modmul(dst, src, mod->rsq, mod);
}

/*
 * Convert a ul128 out-of Montgomery form
 */
inline void ul128_from_montgomery(ul128 dst, ul128 src, mod128 mod) {
    ul128 one = {{{0}}};
    one->x[0] = 1;
    
    ul128_modmul(dst, src, one, mod);
}




/*
 * Right-shift a ul128 by some number of bits
 */
inline void ul128_rshift(ul128 dst, ul128 src, int shift) {
dst->x[0] = (src->x[0] >> shift) | (src->x[1] << (32 - shift));
dst->x[1] = (src->x[1] >> shift) | (src->x[2] << (32 - shift));
dst->x[2] = (src->x[2] >> shift) | (src->x[3] << (32 - shift));
dst->x[3] = dst->x[3] >> shift;
}



/*
 * Left shift a ul128 by some number of words
 */
inline void ul128_lshiftw(ul128 dst, ul128 src, int w) {
    dst->x[3] = ((3-w) >= 0) ? src->x[3-w] : 0;
    dst->x[2] = ((2-w) >= 0) ? src->x[2-w] : 0;
    dst->x[1] = ((1-w) >= 0) ? src->x[1-w] : 0;
    dst->x[0] = ((0-w) >= 0) ? src->x[0-w] : 0;
}

/*
 * Multiply a ul128 by a uint32_t
 */
inline void ul128_mulu32(ul128 dst, ul128 src, uint32_t x) {
    uint64_t x_src_0 = ((uint64_t)src->x[0]) * ((uint64_t)x);
    uint64_t x_src_1 = ((uint64_t)src->x[1]) * ((uint64_t)x);
    uint64_t x_src_2 = ((uint64_t)src->x[2]) * ((uint64_t)x);
    uint64_t x_src_3 = ((uint64_t)src->x[3]) * ((uint64_t)x);
    
    dst->x[0] = 0;
    dst->x[1] = 0;
    dst->x[2] = 0;
    dst->x[3] = 0;
    
    *(uint64_t*)(&dst->x[0]) += x_src_0;
    *(uint64_t*)(&dst->x[1]) += x_src_1;
    *(uint64_t*)(&dst->x[2]) += x_src_2;
    dst->x[3] += x_src_3 >> 32;
}




#endif
